{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Moving Code to the Data: Analyzing Sea Level Rise Using Earth Data in the Cloud\n",
    "\n",
    "Jinbo Wang <Jinbo.Wang@jpl.nasa.gov>, Ed Armstrong, Ian Fenty, Nikki Tebaldi, Jack McNeils, Jonathan Smolenski, Stepheny Perez, Catalina Oaida, Mike Gangl\n",
    "\n",
    "Ackowledgement: PO.DAAC coding club team, [Openscapes](https://www.openscapes.org)\n",
    "\n",
    "July 27, 2022\n",
    "\n",
    "<div style=\"text-align: center\"><img src=\"https://www.earthdata.nasa.gov/s3fs-public/2022-07/PO.DAACWebinar_Banner_7_27_22.png\" width=500 /> </div>\n",
    "\n",
    "## Abstract \n",
    " - Nearly a petabyte of NASA's Physical Oceanography Distributed Active Archive Center (PO.DAAC) data products have been moved to NASA's Earthdata Cloudâ€”hosted in the Amazon Web Services (AWS) cloud. To maximize the full potential of cloud computing on the big Data, one needs to be familiar with not only the data products and their access methods, but also a new set of knowledge for working in a cloud environment. This can be a daunting task for the majority of the science community, who may be familiar with high-performance computing, but not with AWS services. To aid end users in learning and to be successful during this paradigm shift, the PO.DAAC team has been exploring pathways toward practical solutions to help research groups migrate their workflow into cloud.\n",
    "\n",
    " - During this webinar we will explain basic concepts of working in the cloud and use a simple science use case to demonstrate the workflow. Participants do not need prior knowledge of AWS services and the Earthdata Cloud. This is a step-by-step walkthrough of exploring and discovering PO.DAAC data and applying AWS cloud computing to analyze global sea level rise from altimetry data and Estimating the Circulation and Climate of the Ocean (ECCO) products.\n",
    "\n",
    " - We hope that you can start to practice cloud computing using AWS and PODAAC/Earthdata cloud products by following the 6 steps in this tutorial without investing a large amount of time.\n",
    " \n",
    "***\n",
    "## Motivation\n",
    " - It is expected the NASA Earthdata will grow to >250 PB in 2025,\n",
    " - Cloud computing has a big potential\n",
    " - The path to the cloud computing is unclear for majority of the science and application community \n",
    " - We are often perplexed at the start line by a new language related to cloud computing and the large amount of different AWS tools and services, such as CloudFront, EC2, VPC, AMI, IAM, Bucket, Glacier, Snowcone, Snowball, Snowmobile, data lakes, just to name a few.\n",
    " - We aim to share our experience of passing the start line and start to run cloud computing. demonstrate a use case assuming zero knowledge of AWS cloud\n",
    " - The global mean sea level is an important climate indicator, relatively easy to calculate from the PODAAC data in the cloud. \n",
    "\n",
    " \n",
    "## Objectives\n",
    "***\n",
    " - Set up a cloud computing environment from scrath\n",
    " - Run the global mean sea level code in the cloud\n",
    " - Build a webpage server using the same cloud computer \n",
    "\n",
    " \n",
    "### Target audience\n",
    " - Science- and application-oriented group who\n",
    "     - has interest in cloud computing;\n",
    "     - is familiar with python, conda, and jupyter-notebook;\n",
    "     - but with limited knowledge of NASA Earthdata and IT;\n",
    "     - and zero knowledge of AWS cloud.\n",
    "***\n",
    "## Steps toward running in-cloud analysis\n",
    " 1. Get an AWS account\n",
    " 1. Start an AWS cloud computer (Elastic Computer Cloud, EC2) \n",
    "    1. explain AWS console, EC2 instance\n",
    " 1. Configure the EC2 with the necessary software\n",
    "    1. While waiting, explain the global mean sea level analysis code\n",
    " 1. Configure a jupyter-lab on the EC2 and connect to it from browser\n",
    " 1. Demo the code (this notebook) in the cloud and save the figure\n",
    " 1. Set up an apache server (hosting website)\n",
    "    - Create a static html webpage to show the result\n",
    "    \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "There are many ways to achieve this goal. Many alternatives are much smarter but they usually involves a set of new knowledge related to cloud and/or AWS that steepens the learning curve and sometimes makes the process intimidating. The following steps are suggested here because it is believed to involve a minimum amount of specilized knowledge beyond our common practice on our own computer.  \n",
    "</div>\n",
    "\n",
    "### Important terms\n",
    "\n",
    "|AWS terminology | Long name | Meaning|\n",
    "|--|--|--|\n",
    "|AWS Region| |AWS facility. There are many of them. NASA Earth Data are in US-WEST-2, somewhere in Oregon. |\n",
    "|EC2 |Elastic Computer Cloud| A computer in one of the AWS regions. It is a common practice that you should use an EC2 in the region where you data is hosted.|\n",
    "|AWS console| | A web-based control panel for all AWS tools and services. You can start an EC2, create a storage disk (S3 bucket) and much more.  \n",
    "|Key Pair| | An SSH key generated for accessing the EC2, e.g., through SSH. Anyone who has your key can connect to your EC2. It means that you can share the same EC2 with others just through sharing a Key Pair file.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Step 1 -- Get an AWS account\n",
    "***\n",
    "\n",
    "If you already have an AWS account, skip to Step 2. \n",
    "### Choices:\n",
    " 1. Look for institutional support (recommended)\n",
    " 1. Engage in NASA-funded programs (e.g., openscapes)\n",
    " 1. **Apply a free AWS account** (today's focus)\n",
    "    1. It is free for a year but only offers *small* computers (1 CPU, 1GB memory)\n",
    "    1. With the offer of 750 hours per month, a free-tier EC2 can be on all time for a year.\n",
    "    1. Need your personal information including credit card\n",
    "    \n",
    " (https://aws.amazon.com)\n",
    "\n",
    "This page explains the five steps to create an AWS account. \n",
    " > https://progressivecoder.com/creating-an-aws-account-a-step-by-step-process-guide/. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Start an EC2\n",
    "\n",
    "\n",
    "- Log in through aws console https://aws.amazon.com/console/\n",
    " - Start an EC2 (launch instance)\n",
    "   - Name and Tags: earthdata_webinar\n",
    "   - Application and OS Images (Amazon Machine Image): \n",
    "   >**Red Hat Enterprise Linux 8 (HVM) SSD Volume type (free-tier elegible)**\n",
    "   - Instance type: t2.micro (1CPU, 1Gb memory) (If you have a institution- or project-supported AWS account, try to use a bigger computer with >4G memory.)\n",
    "   - Key pair (login): \"Create new key pair\" \n",
    "     - enter a name, e.g., \"aws_ec2_jupyter\"  -> create key pair \n",
    "     - look for the .pem file in the Download folder, move it to .ssh folder. \n",
    "     > ```mv ~/Downloads/aws_ec2_jupyter.pem .ssh/```\n",
    "     - change permission to 400 using \n",
    "     > ```chmod 400 aws_ec2_jupyter.pem```\n",
    "   - check the two boxes for HTTP and HTTPS for the webserver\n",
    "   - Add storage: 10 Gb should be fine for prototyping and testing. You have total 30Gb free storage, which can be split among three EC2s for example. \n",
    "   - Click \"Launch Instance\" button\n",
    "\n",
    " \n",
    "#### Reference\n",
    " - AWS get set up for amazon EC2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/get-set-up-for-amazon-ec2.html\n",
    " - AWS Get started with AWS EC2: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 3 - Login and install the necessary software and prepare EarthData Login (.netrc)\n",
    "\n",
    "1. Find the public IP from your EC2 dashboard.\n",
    "\n",
    "1. First connect to the instance via ssh. \n",
    "```shell\n",
    "     ssh -i \"~/.ssh/aws_ec2_jupyter.pem\" ec2-user@Publica_ip_address -L 9889:localhost:9889\n",
    "```\n",
    "> \n",
    "     Remember to set the following parameters appropriately:\n",
    "     * `-i` points the ssh client on your local machine at your pem key to authenticate\n",
    "     * `-L` tunnels traffic on port `9889` between the ec2 instance and your local machine. This port number can be any value between 1024 and 32767.\n",
    "1. Update packages. Optionally install wget, git etc. for downloading this notebook from github.com\n",
    "     > ```sudo yum update -y && sudo yum install wget -y```\n",
    "1.  Download miniconda install script into *tmp/* and execute it with bash. Then, activate the base environment.\n",
    "```shell\n",
    "  mkdir -p tmp\n",
    "  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O tmp/miniconda.sh && \\\n",
    "  bash tmp/miniconda.sh -b -p $HOME/conda && \\\n",
    "  source ~/conda/bin/activate\n",
    "```\n",
    "1. Create a new environment called *jupyter* running Python 3.7; activate it; install JupyterLab and other required packages.\n",
    "```shell\n",
    "  conda create -n jupyter python=3.7 -y && \\\n",
    "  conda activate jupyter && \\\n",
    "  conda install requests tqdm numpy pandas -y && \\\n",
    "  conda install matplotlib netCDF4 -y &&\\\n",
    "  conda install xarray jupyterlab s3fs hdf5 scipy -y &&\\\n",
    "  conda install pyproj -y\n",
    "```\n",
    "> Warning: the free-tier EC2 has only 1Gb memory. Make sure to monitor the installation to avoid memory errors.\n",
    "1. An EarthData Login (EDL) account is needed for accessing NASA Eartdata regardless the location of the data, either in the Earthdata cloud or on-premise from DAACs. \n",
    "   1. Run the following line in the EC2 terminal:\n",
    "   >```bash\n",
    "echo \"machine urs.earthdata.nasa.gov\\n    login your_earthdata_username\\n    password your_earthdata_account_password\" > ~/.netrc \n",
    "```\n",
    "   1. Use a text editor to replace your_earthdata_username with your EDL username and your_earthdata_account_password with your EDL password.\n",
    "   > ```shell\n",
    "   vi ~/.netrc\n",
    "   ```\n",
    "   1. Change .netrc file permission:\n",
    "   >```shell\n",
    "chmod 400 ~/.netrc\n",
    "```\n",
    "***\n",
    "***\n",
    "### Advanced approach using \"User data\" box to install softwares while launching the EC2 (replacing step 3.3, 3.4, and 3.5)\n",
    "\n",
    "The installation of software can be done by inserting the following bash script into the \"User data\" box during the Launch Instance step (Step 2). It replaces Steps 3.3-3.5. \n",
    "```shell\n",
    "#!/bin/bash\n",
    "  sudo yum update -y\n",
    "  sudo yum install wget -y\n",
    "  sudo yum install httpd -y\n",
    "  sudo service httpd start\n",
    "  mkdir -p ~/tmp\n",
    "  wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O ~/tmp/miniconda.sh\n",
    "  bash ~/tmp/miniconda.sh -b -p /home/ec2-user/conda \n",
    "  source /home/ec2-user/conda/bin/activate\n",
    "  conda update -n base -c defaults conda -y\n",
    "  conda create -n jupyter python=3.7 -y \n",
    "  conda activate jupyter\n",
    "  conda install scipy -y\n",
    "  conda install requests tqdm pandas -y\n",
    "  conda install matplotlib -y \n",
    "  conda install netCDF4 -y\n",
    "  conda install jupyter jupyterlab -y\n",
    "  conda install xarray -y\n",
    "  conda install s3fs -y\n",
    "  conda install pyproj -y\n",
    "```\n",
    " - Make sure to replace the EDL username and password in the .netrc file with your own account.\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Set up a jupyter-lab\n",
    "\n",
    "Jupyterlab is a web-based interactive development environment for python and other languages. It is a perfect tool for accessing the computing resources on an EC2 through SSH tunneling. Jupyterlab and the associated software are install in Step 3. Here is two steps to start and connect to a jupyterlab server on the EC2.\n",
    "\n",
    "1. Use Python to generate and store a hashed password as a shell variable:\n",
    ">\n",
    "```shell\n",
    "PW=\"$(python3 -c 'from notebook.auth import passwd; import getpass; print(passwd(getpass.getpass(), algorithm=\"sha256\"))')\"\n",
    "```\n",
    "1. Start jupyter lab instance with the following parameters:\n",
    ">\n",
    "```shell \n",
    "jupyter lab --port=9889 --ip='127.0.0.1' --NotebookApp.token='' --NotebookApp.password=\"$PW\" --notebook-dir=\"$HOME\" --no-browser```\n",
    "\n",
    "1. Access the server through your web browser: http://127.0.0.1:9889/\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Optional for convinence</b> </br> \n",
    "You can use tmux to start a screen to keep the jupyterlab running on the EC2 even after logging.</br> \n",
    "\n",
    "Detach the screen by pressing CTRL + b -> d. \n",
    "</div>\n",
    "\n",
    "\n",
    "#### Reference\n",
    "\n",
    "* https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html#conda\n",
    "* https://requests.readthedocs.io/en/master/user/install/\n",
    "* https://matplotlib.org/stable/#installation\n",
    "* https://shapely.readthedocs.io/en/latest/\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 5 - Run the code (this notebook) in the cloud and save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data products\n",
    "\n",
    "1. MEaSURES-SSH version JPL1812\n",
    "   - short name: ```SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812```\n",
    "   - [landing page](https://podaac.jpl.nasa.gov/dataset/SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812) (Newer version is available)\n",
    "1. GMSL\n",
    "   - short name: ```JPL_RECON_GMSL```\n",
    "   - [landing page](https://podaac.jpl.nasa.gov/dataset/JPL_RECON_GMSL)\n",
    "1. ECCO global mean sea level \n",
    "   - short name: `ECCO_L4_GMSL_TIME_SERIES_MONTHLY_V4R4`\n",
    "   - [landing page](https://doi.org/10.5067/ECTSM-MSL44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load python modules\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "#Short_name is used to identify a specific dataset in NASA Earthdata. \n",
    "short_name='SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The interface to the AWS Simple Storage Service (S3) file system (stores all NASA Earthdata)\n",
    "\n",
    "PO.DAAC cloud (POCLOUD) is a part of Earthdata Cloud. The data are hosted in a S3 bucket on AWS US-West-2. \"US-West-2\" is a term that refers to the AWS center in Oregon. In this case, the so-called 'Direct-S3 access' only works on the machines hosted in the US-West-2. \n",
    "\n",
    "**s3fs** is a pythonic file interface to S3 built on top of [botocore](https://github.com/boto/botocore). s3fs allows typical file-system style operations like cp, mv, ls, du, glob, and put/get of local files to/from S3. Details can be find on its website https://s3fs.readthedocs.io/en/latest/.  \n",
    "\n",
    "It is important that you set up the .netrc file correctly in order to enable the following *init_S3FileSystem* module. The .netrc file should be placed in your home folder. A typical .netrc file has the following content:\n",
    "```bash\n",
    "machine urs.earthdata.nasa.gov\n",
    "    login your_earthdata_username\n",
    "    password your_earthdata_account_password\n",
    " ```\n",
    " \n",
    "If you do not have or do not remember your Earthdata Login information, go [here](https://urs.earthdata.nasa.gov/users/new) to register or [here](https://urs.earthdata.nasa.gov/reset_passwords/new) to reset password. \n",
    "\n",
    "###  AWS credentials with EDL\n",
    "\n",
    "AWS requires security credentials to access AWS S3.  \n",
    "\n",
    "With your EDL, you can obtain a temporay S3 credential through https://archive.podaac.earthdata.nasa.gov/s3credentials. It is a 'digital key' to access the Earthdata in AWS cloud. Here is an example:\n",
    "\n",
    "> {\"accessKeyId\": \"xxxx\", \"secretAccessKey\": \"xxxx\", \"sessionToken\": \"xxxx\", \"expiration\": \"2022-07-22 15:56:34+00:00\"}\n",
    "\n",
    "Further reading: https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_S3FileSystem():\n",
    "    import requests,s3fs\n",
    "    creds = requests.get('https://archive.podaac.earthdata.nasa.gov/s3credentials').json()\n",
    "    s3 = s3fs.S3FileSystem(anon=False,\n",
    "                           key=creds['accessKeyId'],\n",
    "                           secret=creds['secretAccessKey'], \n",
    "                           token=creds['sessionToken'])\n",
    "    return s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use s3fs.glob to get all file names\n",
    "\n",
    "The S3FileSystem allows typical file-system style operations like `cp, mv, ls, du, glob`. Once the s3fs file system is established, we can use 'glob' to get all file names from a collection. In this case, the collection S3 path is \n",
    "```bash\n",
    "s3://podaac-ops-cumulus-protected/SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812/\n",
    "```\n",
    "\n",
    "Using the following will get a list netcdf filenames: \n",
    "```\n",
    "fns=s3sys.glob(\"s3://podaac-ops-cumulus-protected/SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812/*.nc\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3sys=init_S3FileSystem()\n",
    "\n",
    "s3path=\"s3://podaac-ops-cumulus-protected/%s/\"%short_name\n",
    "fns=s3sys.glob(s3path+\"*.nc\")\n",
    "print(fns[0])\n",
    "#Set the time stamps associated with the files\n",
    "time=pd.date_range(start='1992-10-02',periods=len(fns),freq='5D') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %i files.'%len(fns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=xr.open_dataset(s3sys.open(fns[0]))\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.contourf(d['Longitude'],d['Latitude'],d['SLA'][0,...].T,levels=np.arange(-0.5,0.6,0.05))\n",
    "plt.ylabel('Latitude')\n",
    "plt.xlabel('Longitude')\n",
    "plt.title('Sea Level Anomaly %s'%d.time_coverage_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the global mean SSHA\n",
    "\n",
    "The global mean SSH is calculated as follows. \n",
    "\n",
    "$SSH_{mean} = \\sum \\eta(\\phi,\\lambda)*A(\\phi)$, where $\\phi$ is latitude, $\\lambda$ is longitude, $A$ is the area of the grid at latitude $\\phi$, and $\\eta(\\phi,\\lambda)*A(\\phi)$ is the weighted SLA at $(\\phi,\\lambda)$. \n",
    "\n",
    "The following routine `area` pre-calculates the area as a function of latitude for the 1/6-degree resolution grids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area(lats):\n",
    "    \"\"\"\n",
    "    Calculate the area associated with a 1/6 by 1/6 degree box at latitude specified in 'lats'. \n",
    "    \n",
    "    Parameter\n",
    "    ==========\n",
    "    lats: a list or numpy array of size N\n",
    "          the latitudes of interest. \n",
    "    \n",
    "    Return\n",
    "    =======\n",
    "    out: Array (N)\n",
    "         area values (unit: m^2)\n",
    "    \"\"\"\n",
    "    # Modules:\n",
    "    from pyproj import Geod\n",
    "    # Define WGS84 as CRS:\n",
    "    geod = Geod(ellps='WGS84')\n",
    "    dx=1/12.0\n",
    "    c_area=lambda lat: geod.polygon_area_perimeter(np.r_[-dx,dx,dx,-dx], lat+np.r_[-dx,-dx,dx,dx])[0]\n",
    "    out=[]\n",
    "    for lat in lats:\n",
    "        out.append(c_area(lat))\n",
    "    return np.array(out)\n",
    "\n",
    "def global_mean(fn_s3,s3sys,ssh_area):\n",
    "    \"\"\"\n",
    "    Calculate the global mean given an s3 file of SSH, a s3fs.S3FileSystem, \n",
    "    and the ssh_area, which is precalculated to save computing time. \n",
    "    Parameter:\n",
    "    ===========\n",
    "    fn_s3: S3 file name, e.g., s3://podaac-ops-cumulus-protected/SEA_SURFACE_HEIGHT_ALT_GRIDS_L4_2SATS_5DAY_6THDEG_V_JPL1812/ssh_grids_v1812_1992100212.nc\n",
    "    s3sys: generated by s3fs.S3FileSystem\n",
    "    ssh_area: the area size associated with MEaSURES-SSH 1/6-degree resolution product. \n",
    "    \n",
    "    Return\n",
    "    =======\n",
    "    dout: scalar\n",
    "          The global mean sea level (default unit from MEaSURES-SSH: meter)\n",
    "    \"\"\"\n",
    "    with xr.open_dataset(s3sys.open(fn_s3))['SLA'] as d:\n",
    "        dout=((d*ssh_area).sum()/(d/d*ssh_area).sum()).values\n",
    "    return dout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=xr.open_dataset(s3sys.open(fns[0]))\n",
    "#pre-calculate the area for reuse\n",
    "ssh_area=area(d.Latitude.data).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The global mean sea level from %s is %7.5f meters.'%(fns[0],global_mean(fns[0],s3sys,ssh_area) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate using a single thread\n",
    "\n",
    "Benchmark: using a sigle thread takes about 17 min to calculate all 1922 files. Here the program is sped up by skipping every 360 days. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#Loop 26-year 5-daily SSH fields (1922 files)\n",
    "#Skip every 72 files to speed up\n",
    "\n",
    "result=[]\n",
    "t_local=time[::72]\n",
    "for fn in fns[::72]:\n",
    "    result.append(global_mean(fn,s3sys,ssh_area)*1e3 )\n",
    "result=np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(t_local,result-10,'r-o')\n",
    "tyr=(t_local-t_local[0])/np.timedelta64(1,'Y') #convert the number of years\n",
    "msk=np.isnan(result)\n",
    "tyr=tyr[~msk]\n",
    "result=result[~msk]\n",
    "\n",
    "#Calculate the linear trend using linear regression `linregress`\n",
    "rate=linregress(tyr[1:],result[1:]) \n",
    "print('The estimated sea level rise rate between 1993 and 2018: %5.1fmm/year.'%(rate[0]) )\n",
    "plt.text(2004,10, 'linear trend: %5.1fmm/year'%(rate[0]) )\n",
    "plt.xlabel('Time (year)',fontsize=16)\n",
    "plt.ylabel('Global Mean SLA (mm)',fontsize=16)\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Quiz</b> </br> \n",
    "The global sea level trend from altimetry should be 3.0mm/year. Why did we get 2.5mm/year from the above analysis? Can you get 3.0mm/year by modifying the above code?\n",
    "    \n",
    "<b>Hint</b>: The above analysis is aliased. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the GMSL from ECCO V4R4 \n",
    "Skipped for this webinar but will be added in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the GMSL from Frederikse et al. https://podaac.jpl.nasa.gov/dataset/JPL_RECON_GMSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(t_local,result-10,'r-o',label='altimetry')\n",
    "\n",
    "plt.xlabel('Time (year)',fontsize=16)\n",
    "plt.ylabel('Global Mean SLA (meter)',fontsize=16)\n",
    "plt.grid(True)\n",
    "\n",
    "# Add GMSL from \n",
    "\n",
    "d1=xr.open_dataset('https://opendap.jpl.nasa.gov/opendap/allData/homage/L4/gmsl/global_timeseries_measures.nc')\n",
    "print(d1)\n",
    "d1['global_average_sea_level_change'].plot(label='in-situ')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('gmsl.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Quiz</b> </br> \n",
    "The global sea level trend from tide-gauge reconstruction (3.5mm/year) is steeper than altimetry-based analysis (3.0mm/year). Why is that?\n",
    "\n",
    "<b>Hint</b>: Altimetry-based analysis does not consider vertical land motion. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 6 (optional) -- Run an apache web server to show the result\n",
    "\n",
    "From your EC2 command line, install apache webserver: \n",
    "\n",
    ">\n",
    "```shell\n",
    "sudo yum install httpd -y\n",
    "```\n",
    "Start the server and auto-start when stopped\n",
    "\n",
    ">\n",
    "```shell\n",
    "sudo service httpd start\n",
    "```\n",
    "Copy and paste the following code to make a webpage `index.html` \n",
    "\n",
    "```html\n",
    "<html>\n",
    "<head>\n",
    "    <center>\n",
    "     <h1 style=\"font-size:30px\">The global mean sea level </h1>\n",
    "     <h2 style=\"font-size:20px\">Hosted on Jinbo Wang's personal AWS EC2</h2>\n",
    "     <img src=\"gmsl.png\" alt=\"Global Mean Sea Level\" width=\"700\">\n",
    "     <h1 style=\"font-size:20px\">Diagnosed from MEaSURES-SSH (red) and JPL_RECON_GMSL (blue)</h1>\n",
    "     <h1 style=\"font-size:20px\">Earthdata webinar, 07/27/2022</h1>\n",
    "     <h1 style=\"font-size:20px;color=purple\">Cloud-based analysis is fun!</h1>\n",
    "     <img src=\"https://chucktownfloods.cofc.edu/wp-content/uploads/2019/07/Earthdata-Logo.jpg\" width=\"200\">\n",
    "    </center>\n",
    "</head>\n",
    "</html>\n",
    "```\n",
    "Move `index.html` to the default location `/var/www/html/` using \n",
    ">\n",
    "```shell\n",
    "sudo cp index.html /var/www/html/\n",
    "```\n",
    "Make sure to use `cp` not `mv` to change the ownership to `root` (just for this demo, it is not the good practice to user root for this).  \n",
    "\n",
    "Copy ```gmsl.png``` into ```/var/www/html/```.\n",
    "\n",
    "\n",
    ">```shell\n",
    "sudo cp gmsl.png /var/www/html/\n",
    "\n",
    "Access the webpage through the EC2 IP address from browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Summary\n",
    "\n",
    " - Online materials for using AWS cloud and Eararthdata are abundant but often oranized by topics.\n",
    " - Here we focus on building a simple workflow from scratch to show how in-cloud analysis can be achieved with minimal knowledge of AWS cloud\n",
    " - By repeating these steps, one is anticipated to learn the basic concepts of AWS and in-cloud analysis as well as PODAAC/Earthdata cloud. \n",
    "\n",
    "### Conclusions\n",
    "\n",
    " - Apply cloud-computing is not difficult, but finding the right path is. \n",
    " - Relying on free AWS account linked to personal finance is not sustainable. The community needs a clear official instruction on the channels of getting supported. \n",
    " - Support for small-size proposals are needed to advance cloud computing from early adopters to mainstream. \n",
    "\n",
    "### Lesson Learned\n",
    " - Learn as a group\n",
    "   - Small-size 'coding-clubs' with scienists and engineers is helpful to solve problems faster. \n",
    " - Start from basics\n",
    " - Learn cloud by solving a practical problem, for example:\n",
    "   - I would like to analyze global mean sea level in the cloud\n",
    "   - I would like to build a regional sea level rise indictor in the cloud and host the result realtime through a website\n",
    "   - I would like to build a notebook to show diverse satellite and in-situ data to support a field campaign in realtime. \n",
    " - Restricted cyber environment needs more attention to the Virtual Private Cloud (VPC) configuration. (We wasted many months on this item.)\n",
    " \n",
    "### Future development\n",
    "  - Scale-up analysis in the cloud\n",
    "    - AWS Lambda\n",
    "    - AWS Batch\n",
    "    - AWS HPC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Extra materials\n",
    "(To be completed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Scale-up: using Dask delayed to parallel process\n",
    "\n",
    "Benchmark: It took 44 seconds to go through the 1922 files using 8 workers on an instance c3.xlarge. \n",
    "\n",
    "The previous steps did not install dask. Dask can be installed using conda:\n",
    "\n",
    "```bash\n",
    " conda install dask\n",
    "```\n",
    "\n",
    "Also consider install dask-extension for the jupyterlab. \n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge nodejs\n",
    "pip install dask-labextension\n",
    "```\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask import delayed,compute\n",
    "client = Client('tcp://127.0.0.1:44989')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "result=[]\n",
    "\n",
    "for fn in fns:\n",
    "    result.append(delayed(global_mean)(fn,s3sys,ssh_area) )\n",
    "\n",
    "output=np.array(compute(result)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.plot(time,output,'r--')\n",
    "plt.xlabel('Time (year)',fontsize=16)\n",
    "plt.ylabel('Global Mean SLA (meter)',fontsize=16)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
